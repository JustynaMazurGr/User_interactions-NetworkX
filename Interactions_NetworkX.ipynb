{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Looking for the sum of disconnected groups of user interactions in different game categories based on their activity under videos."
      ],
      "metadata": {
        "id": "_00-Gtq88xBU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyspark"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yll2T662Hife",
        "outputId": "5c204eda-d7d9-4476-a96e-1c8428b12842"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyspark\n",
            "  Downloading pyspark-3.5.0.tar.gz (316.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m316.9/316.9 MB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n",
            "Building wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.5.0-py2.py3-none-any.whl size=317425345 sha256=80fc76a3790ebbd4bbe488a8b9d5248b3362dc41e0722d29d9e6426c6019026a\n",
            "  Stored in directory: /root/.cache/pip/wheels/41/4e/10/c2cf2467f71c678cfc8a6b9ac9241e5e44a01940da8fbb17fc\n",
            "Successfully built pyspark\n",
            "Installing collected packages: pyspark\n",
            "Successfully installed pyspark-3.5.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder\\\n",
        "        .master(\"local\")\\\n",
        "        .appName(\"colab\")\\\n",
        "        .getOrCreate()"
      ],
      "metadata": {
        "id": "gCcPiIJxHlKr"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark import SparkFiles\n",
        "from pyspark.sql import functions as sf\n",
        "from pyspark.sql.window import Window\n",
        "import networkx as nx"
      ],
      "metadata": {
        "id": "EOMtiPmyHqSU"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "spark.sparkContext.addFile(\"/content/games.csv\")\n",
        "spark.sparkContext.addFile(\"/content/game_movies.csv\")\n",
        "spark.sparkContext.addFile(\"/content/game_groups.csv\")\n",
        "spark.sparkContext.addFile(\"/content/comments.csv\")\n",
        "\n",
        "games = spark.read.csv(\"file:///\"+SparkFiles.get(\"/content/games.csv\"), header = True, inferSchema = True)\n",
        "game_movies = spark.read.csv(\"file:///\"+SparkFiles.get(\"/content/game_movies.csv\"), header = True, inferSchema = True)\n",
        "game_groups = spark.read.csv(\"file:///\"+SparkFiles.get(\"/content/game_groups.csv\"), header = True, inferSchema = True)\n",
        "comments = spark.read.csv(\"file:///\"+SparkFiles.get(\"/content/comments.csv\"), header = True, inferSchema = True)"
      ],
      "metadata": {
        "id": "IuluCxdpHuSU"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZycS4lHK8pJe",
        "outputId": "451d98da-f280-4ac7-b951-601cbc75f374"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "37\n"
          ]
        }
      ],
      "source": [
        "# Merging DataFrames, grouping, and sorting by id_movie\n",
        "merged_data_game_group = comments.join(game_movies, 'id_movie', 'inner') \\\n",
        "                                  .join(games, 'id_game', 'inner') \\\n",
        "                                  .join(game_groups, 'id_game_group', 'inner') \\\n",
        "                                  .orderBy('id_movie')\n",
        "\n",
        "# Window specifying data partitioning for each game group\n",
        "window_spec = Window.partitionBy(\"id_game_group\")\n",
        "\n",
        "# Counting unique pairs (movie, user) within each game group\n",
        "edges = merged_data_game_group.select(\"id_movie\", \"user\", \"id_game_group\") \\\n",
        "    .groupBy(\"id_movie\", \"user\", \"id_game_group\") \\\n",
        "    .agg(sf.count(\"*\").over(window_spec).alias(\"interaction_count\")) \\\n",
        "    .filter(sf.col(\"interaction_count\") > 1) \\\n",
        "    .groupBy(\"id_game_group\", \"id_movie\") \\\n",
        "    .agg(sf.collect_list(\"user\").alias(\"users\"))\n",
        "\n",
        "# Creating a graph\n",
        "def create_graph(rows):\n",
        "    G = nx.Graph()\n",
        "    for row in rows:\n",
        "        users = row['users']\n",
        "        unique_users = list(set(users))\n",
        "        for i in range(len(unique_users)):\n",
        "            for j in range(i + 1, len(unique_users)):\n",
        "                user1 = unique_users[i]\n",
        "                user2 = unique_users[j]\n",
        "                G.add_edge(user1, user2)\n",
        "    return G\n",
        "\n",
        "# Applying the function to each partition\n",
        "graph_per_group = edges.rdd.groupBy(lambda x: x[\"id_game_group\"]).mapValues(create_graph)\n",
        "\n",
        "# Counting the number of interaction groups for each game group\n",
        "count_of_interactions_per_group = graph_per_group.mapValues(lambda G: len(list(nx.connected_components(G))))\n",
        "\n",
        "# Summing the number of interaction groups for all game groups\n",
        "sum_of_interactions_per_group = count_of_interactions_per_group.map(lambda x: x[1]).sum()\n",
        "\n",
        "# Displaying the result\n",
        "print(sum_of_interactions_per_group)\n"
      ]
    }
  ]
}